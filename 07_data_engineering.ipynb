{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering\n",
    "\n",
    "-It is the practice of designing and building systems for collecting, storing, and analyzing data at scale.Basically Data Engineering handles data makes it accessible (maintain, organize and store huge amounts of incoming data in databases) to Data Science disciplines/techniques (AI, Machine Learning, Deep Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is Data\n",
    "\n",
    "* *Structured Data*: Usually comes from structural databases e.g. MySQL, PostgreSQL \n",
    "* *Semi-structured Data*:e.g XMS,CSV, JSON\n",
    "* *Unstructured Data*: e.g PDF, e-mails, any type of documents\n",
    "* *Binary Data*: MP3, MP4 (audio/video files), image files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Data Mining**: also known as knowledge discovery in data (KDD), is the process of uncovering patterns and other valuable information from large data sets. Preprocessing and extracting some knowledge from the data\n",
    "\n",
    "- **Big Data**: extremely large and diverse collections of structured, unstructured, and semi-structured data that continues to grow exponentially over time. These datasets are so huge and complex in volume, velocity, and variety, that traditional data management systems cannot store, process, and analyze them. huge amount of data that conveniently used processed on cloud or multiple computers\n",
    "\n",
    "- **Data Pipeline**: is a method in which raw data is ingested from various data sources and then ported to data store, like a data lake or data warehouse, for analysis. Before data flows into a data repository, it usually undergoes some data processing. This is inclusive of data transformations, such as filtering, masking, and aggregations, which ensure appropriate data integration and standardization. \n",
    "A pipeline/method data engineer builds to extract and store in data bases useful form of data or information from huge amounts of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data lake**: A data lake is a centralized repository that allows you to store all your structured and unstructured data (stored in its natural/raw format, usually object blobs or files) at any scale. You can store your data as-is, without having to first structure the data, and run different types of analytics—from dashboards and visualizations to big data processing, real-time analytics, and machine learning to guide better decisions.\n",
    "\n",
    "**Data warehouse**: A data warehouse, is a system that aggregates data from different sources into a single, central, consistent data store to support data analysis, data mining, artificial intelligence (AI), and machine learning. A data warehouse system enables an organization to run powerful analytics on huge volumes (petabytes and petabytes) of historical data in ways that a standard database cannot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ETL Pipeline**: (Extract, Transform, Load)  is a three-phase process where data is extracted, transformed (cleaned, sanitized, scrubbed) and loaded into an output data container. The data can be collated from one or more sources and it can also be output to one or more destinations.\n",
    "\n",
    " Data Engineer's main tasks: \n",
    "    - Create ETL pipelines \n",
    "    - Build analysis tools ( to analyze the data and also make sure the system is running correctly)\n",
    "    - Maintain data warehouses and data lakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Types of Databases\n",
    "- **Relational Database**: allows you to use SQL to make transactions, PostgreSQL, MySQL (ACID: Atomicity, Consistency, Isolation, and Durability) https://www.yugabyte.com/acid/\n",
    "- **NoSQL**: distributed databases, multiple databases working together, MongoDB\n",
    "-**NewSQL**: combines advantages of both database types, has the distributed nature of NoSQLs and reliability of Relationals, VaultDB, CockroachDb\n",
    "- *Databases according to their best uses*: Some DB provides very efficient *Search* option (Elasticsearch or Solar), Some DB provides very efficient *Computation* option (Apache Spark)\n",
    "\n",
    "- **OLTP**: Online Transactional Processing, the main focus of OLTP system is to record the current Update, Insertion and Deletion while transaction. The OLTP queries are simpler and short and hence require less time in processing, and also requires less space.\n",
    "\n",
    "\n",
    "- **OLAP**: Onine Analytical Processing, OLAP database stores historical data that has been inputted by OLTP. It allows a user to view different summaries of multi-dimensional data. Using OLAP, you can extract information from a large database and analyze it for decision making.\n",
    "https://techdifferences.com/difference-between-oltp-and-olap.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. OLTP Databases\n",
    "- DBMS: Database Management System\n",
    "- Relational and non-relational databases are two methods of data storage for applications. \n",
    "- Relational database (or SQL database) stores data in tabular format with rows and columns. The columns contain data attributes and the rows have data values. You can link the tables in a relational database to gain deeper insights into the interconnection between diverse data points.\n",
    "- Non-relational databases (or NoSQL databases) use a variety of data models for accessing and managing data. They are optimized specifically for applications that require large data volume, low latency, and flexible data models, which is achieved by relaxing some of the data consistency restrictions of other databases. In effect, non-relational databases are designed to contain unstructured data, or loosely defined data like email messages, videos, images, and business documents that aren’t easily standardized. They can also be used to store a mix of structured and unstructured data. \n",
    "    \n",
    "    - Relational Databases: PostgreSQL, MySQL, SQLite, Microsoft SQL Server...\n",
    "    - NoSQL/Non Relational Databases: mongoDB, CouchDB, redis..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Engineering Tools\n",
    "\n",
    "*Below are some of the Data Engineering Tools*\n",
    "\n",
    "#### 3.1 Hadoop, HDFS, MapReduce\n",
    "\n",
    "\n",
    "\n",
    "- **hadoop**: is open source distributed processing framework, The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures. https://hadoop.apache.org/\n",
    "\n",
    "-**HDFS**: Hadoop Distributed File System, allowing to store files on multiple computers,  is a distributed file system that handles large data sets running on commodity hardware. It is used to scale a single Apache Hadoop cluster to hundreds (and even thousands) of nodes. HDFS is one of the major components of Apache Hadoop, the others being MapReduce and YARN.\n",
    "\n",
    "- **MapReduce**: (with Hadoop allows processing or running batch jobs on data in a data lake )MapReduce is a programming paradigm that enables massive scalability across hundreds or thousands of servers in a Hadoop cluster. As the processing component, MapReduce is the heart of Apache Hadoop. The term \"MapReduce\" refers to two separate and distinct tasks that Hadoop programs perform. The first is the map job, which takes a set of data and converts it into another set of data, where individual elements are broken down into tuples (key/value pairs).\n",
    "\n",
    "The reduce job takes the output from a map as input and combines those data tuples into a smaller set of tuples. As the sequence of the name MapReduce implies, the reduce job is always performed after the map job.\n",
    "\n",
    "- **Hive**: The Apache Hive ™ is a distributed, fault-tolerant data warehouse system that enables analytics at a massive scale and facilitates reading, writing, and managing petabytes of data residing in distributed storage using SQL. https://hive.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Apache Spark and Apache Flink\n",
    "\n",
    "- **Apache Spark**: (an improved tool than Hadoop MapReduce thanks to its In-memory processing feature) Apache Spark™ is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters. https://spark.apache.org/ \n",
    "\n",
    "- **Apache Flink**: (offers real-time stream processing) Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. https://flink.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Kafka and Stream Processing\n",
    "\n",
    "**Kafka**: (allows read/write, processing, storing data) Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications. https://kafka.apache.org/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
